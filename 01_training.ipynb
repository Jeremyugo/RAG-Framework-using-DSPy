{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dspy/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from rich import print\n",
    "from ragatouille import RAGTrainer\n",
    "\n",
    "trainer = RAGTrainer(\n",
    "    model_name=\"JerryColBERT\",\n",
    "    pretrained_model_name=\"colbert-ir/colbertv2.0\",\n",
    "    language_code=\"en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data/term_paper.txt\")\n",
    "my_full_corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille.data import CorpusProcessor, llama_index_sentence_splitter\n",
    "\n",
    "corpus_processor = CorpusProcessor(\n",
    "    document_splitter_fn=llama_index_sentence_splitter)\n",
    "documents = corpus_processor.process_corpus(my_full_corpus, chunk_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "queries = [\n",
    "    \"Why is there a growing need for machine learning in petroleum engineering?\",\n",
    "    \"Why was early stopping used during training of the CONV-LSTM model for oil production rate prediction?\",\n",
    "    \"What were the primary findings of the case study regarding the application of deep learning models for predicting oil production rates?\",\n",
    "    \"In the context of the case study, what is the significance of the learning curve shown in Figure 12?\",\n",
    "] * 3\n",
    "\n",
    "pairs = []\n",
    "for query in queries:\n",
    "    fake_relevant_docs = random.sample(documents, 10)\n",
    "    for doc in fake_relevant_docs:\n",
    "        pairs.append((query, doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Hard Negative SimpleMiner dense embedding model BAAI/bge-small-en-v1.5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dspy/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building hard negative index for 89 documents...\n",
      "All documents embedded, now adding to index...\n",
      "save_index set to False, skipping saving hard negative index\n",
      "Hard negative index generated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./data/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.prepare_training_data(\n",
    "    raw_data=pairs,\n",
    "    data_out_path=\"./data/\",\n",
    "    all_documents=my_full_corpus,\n",
    "    num_new_negatives=10,\n",
    "    mine_hard_negatives=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> Starting...\n",
      "nranks = 1 \t num_gpus = 1 \t device=0\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"index_bsize\": 64,\n",
      "    \"nbits\": 4,\n",
      "    \"kmeans_niters\": 20,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 32,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 5e-6,\n",
      "    \"maxsteps\": 500000,\n",
      "    \"save_every\": 0,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": \"JerryColBERT\",\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 256,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"colbert-ir\\/colbertv2.0\",\n",
      "    \"triples\": \"data\\/triples.train.colbert.jsonl\",\n",
      "    \"collection\": \"data\\/corpus.train.colbert.tsv\",\n",
      "    \"queries\": \"data\\/queries.train.colbert.tsv\",\n",
      "    \"index_name\": null,\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \".ragatouille\\/\",\n",
      "    \"experiment\": \"colbert\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-05\\/11\\/23.00.08\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 1,\n",
      "    \"avoid_fork_if_possible\": false\n",
      "}\n",
      "Using config.bsize = 32 (per process) and config.accumsteps = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dspy/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[May 11, 23:00:26] #> Loading the queries from data/queries.train.colbert.tsv ...\n",
      "[May 11, 23:00:26] #> Got 4 queries. All QIDs are unique.\n",
      "\n",
      "[May 11, 23:00:26] #> Loading collection...\n",
      "0M \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dspy/lib/python3.9/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> LR will use 0 warmup steps and linear decay over 500000 steps.\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . What were the primary findings of the case study regarding the application of deep learning models for predicting oil production rates?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2054,  2020,  1996,  3078,  9556,  1997,  1996,  2553,\n",
      "         2817,  4953,  1996,  4646,  1997,  2784,  4083,  4275,  2005, 29458,\n",
      "         3514,  2537,  6165,  1029,   102,   103,   103,   103,   103,   103,\n",
      "          103,   103], device='cuda:0')\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "\n",
      "\t\t\t\t 0.9447746872901917 4.579592227935791\n",
      "#>>>    1.48 1.47 \t\t|\t\t 0.010000000000000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dspy/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[May 11, 23:00:38] 0 5.524366855621338\n",
      "\t\t\t\t 1.0428612232208252 4.827328205108643\n",
      "#>>>    1.26 1.49 \t\t|\t\t -0.22999999999999998\n",
      "[May 11, 23:00:41] 1 5.524712678432464\n",
      "[May 11, 23:00:41] #> Done with all triples!\n",
      "#> Saving a checkpoint to .ragatouille/colbert/none/2024-05/11/23.00.08/checkpoints/colbert ..\n",
      "#> Joined...\n"
     ]
    }
   ],
   "source": [
    "trainer.train(batch_size=32,\n",
    "              nbits=4, # How many bits will the trained model use when compressing indexes\n",
    "              maxsteps=500000, # Maximum steps hard stop\n",
    "              use_ib_negatives=True, # Use in-batch negative to calculate loss\n",
    "              dim=128, # How many dimensions per embedding. 128 is the default and works well.\n",
    "              learning_rate=5e-6, # Learning rate, small values ([3e-6,3e-5] work best if the base model is BERT-like, 5e-6 is often the sweet spot)\n",
    "              doc_maxlen=256, # Maximum document length. Because of how ColBERT works, smaller chunks (128-256) work very well.\n",
    "              use_relu=False, # Disable ReLU -- doesn't improve performance\n",
    "              warmup_steps=\"auto\", # Defaults to 10%\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
