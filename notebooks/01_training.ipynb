{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval model training\n",
    "\n",
    "In this notebook, we'll train a custom retrieval model for our RAG framework using the pretrained ColBERT model. \n",
    "\n",
    "The ColBERT retrieval model generalizes well for most usecases, however there might be a need to train our own custom retrieval model when working with proprietary data, and this notebook shows how to do so.\n",
    "\n",
    "The data used here is gotten from Wikipedia, however the same process can be replicated for proprietary datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing relevant packages\n",
    "from rich import print\n",
    "import random\n",
    "import requests\n",
    "from ragatouille import RAGTrainer\n",
    "from ragatouille.data import CorpusProcessor, llama_index_sentence_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the trainer\n",
    "trainer = RAGTrainer(\n",
    "    model_name=\"EPLColBERT\",\n",
    "    pretrained_model_name=\"colbert-ir/colbertv2.0\",\n",
    "    language_code=\"en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: We can use proprietary datasets, however, due to safety reasons i will be using a publicly available dataset from Wikipedia.\n",
    "\n",
    "To use a proprietary dataset, all you need to do is load and preprocess the dataset to a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data from wikipedia using the API\n",
    "def get_wikipedia_page(title: str):\n",
    "    \"\"\"\n",
    "    Retrieve the full text content of a Wikipedia page.\n",
    "    \n",
    "    :param title: str - Title of the Wikipedia page.\n",
    "    :return: str - Full text content of the page as raw string.\n",
    "    \"\"\"\n",
    "    # Wikipedia API endpoint\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "    }\n",
    "\n",
    "    # Custom User-Agent header to comply with Wikipedia's best practices\n",
    "    headers = {\n",
    "        \"User-Agent\": \"RAGatouille_tutorial/0.0.1 (ben@clavie.eu)\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(URL, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extracting page content\n",
    "    page = next(iter(data['query']['pages'].values()))\n",
    "    return page['extract'] if 'extract' in page else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the data\n",
    "epl_corpus = [get_wikipedia_page('Manchester United F.C.'), get_wikipedia_page('Manchester City F.C.'), get_wikipedia_page('Arsenal F.C.'), get_wikipedia_page('Chelsea F.C.'), get_wikipedia_page('Tottenham Hotspur F.C.'), get_wikipedia_page('Liverpool F.C.'), get_wikipedia_page('Premier League')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when building RAGs, its advisable to split the data into smaller chunks\n",
    "corpus_processor = CorpusProcessor(\n",
    "    document_splitter_fn=llama_index_sentence_splitter)\n",
    "documents = corpus_processor.process_corpus(epl_corpus, chunk_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating random query-document pairs (really doesn't matter if its correct)\n",
    "queries = [\n",
    "    \"When was Manchester United formed?\",\n",
    "    \"How many english league titles have Liverpool won?\",\n",
    "    \"Which club is the oldest in England?\",\n",
    "    \"Which club has the record for the longest unbeaten run in the premier league?\",\n",
    "    \"Who is the most successful manager in the premier league?\"\n",
    "] * 3\n",
    "\n",
    "pairs = []\n",
    "\n",
    "for query in queries:\n",
    "    fake_docs = random.sample(documents, 10)\n",
    "    for doc in fake_docs:\n",
    "        pairs.append((query, doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparing our data/pairs by performing hard negative mining, which searches for the entire documents for passages that are semantically similar to the query, but aren't actually relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Hard Negative SimpleMiner dense embedding model BAAI/bge-small-en-v1.5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dspy2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building hard negative index for 136 documents...\n",
      "All documents embedded, now adding to index...\n",
      "save_index set to False, skipping saving hard negative index\n",
      "Hard negative index generated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../data/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.prepare_training_data(\n",
    "    raw_data=pairs,\n",
    "    data_out_path=\"../data/\",\n",
    "    all_documents=epl_corpus,\n",
    "    num_new_negatives=10,\n",
    "    mine_hard_negatives=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> Starting...\n",
      "nranks = 1 \t num_gpus = 1 \t device=0\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"index_bsize\": 64,\n",
      "    \"nbits\": 4,\n",
      "    \"kmeans_niters\": 20,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 32,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 5e-6,\n",
      "    \"maxsteps\": 500000,\n",
      "    \"save_every\": 0,\n",
      "    \"warmup\": 0,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": \"EPLColBERT\",\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 128,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"colbert-ir\\/colbertv2.0\",\n",
      "    \"triples\": \"..\\/data\\/triples.train.colbert.jsonl\",\n",
      "    \"collection\": \"..\\/data\\/corpus.train.colbert.tsv\",\n",
      "    \"queries\": \"..\\/data\\/queries.train.colbert.tsv\",\n",
      "    \"index_name\": null,\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \".ragatouille\\/\",\n",
      "    \"experiment\": \"colbert\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-05\\/13\\/01.27.16\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 1,\n",
      "    \"avoid_fork_if_possible\": false\n",
      "}\n",
      "Using config.bsize = 32 (per process) and config.accumsteps = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dspy2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[May 13, 01:27:38] #> Loading the queries from ../data/queries.train.colbert.tsv ...\n",
      "[May 13, 01:27:38] #> Got 5 queries. All QIDs are unique.\n",
      "\n",
      "[May 13, 01:27:38] #> Loading collection...\n",
      "0M \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dspy2/lib/python3.9/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> LR will use 0 warmup steps and linear decay over 500000 steps.\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . Who is the most successful manager in the premier league?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([ 101,    1, 2040, 2003, 1996, 2087, 3144, 3208, 1999, 1996, 4239, 2223,\n",
      "        1029,  102,  103,  103,  103,  103,  103,  103,  103,  103,  103,  103,\n",
      "         103,  103,  103,  103,  103,  103,  103,  103], device='cuda:0')\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "\n",
      "\t\t\t\t 6.6913957595825195 11.028946876525879\n",
      "#>>>    10.19 16.7 \t\t|\t\t -6.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/dspy2/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[May 13, 01:28:06] 0 17.7203426361084\n",
      "\t\t\t\t 3.913029432296753 7.930639266967773\n",
      "#>>>    13.98 17.03 \t\t|\t\t -3.0500000000000007\n",
      "[May 13, 01:28:33] 1 17.714465962409975\n",
      "\t\t\t\t 6.548213005065918 11.229314804077148\n",
      "#>>>    9.65 16.06 \t\t|\t\t -6.409999999999998\n",
      "[May 13, 01:28:59] 2 17.714529023303033\n",
      "\t\t\t\t 7.281060218811035 12.035734176635742\n",
      "#>>>    10.4 17.65 \t\t|\t\t -7.249999999999998\n",
      "[May 13, 01:29:27] 3 17.71613128962885\n",
      "[May 13, 01:29:27] #> Done with all triples!\n",
      "#> Saving a checkpoint to .ragatouille/colbert/none/2024-05/13/01.27.16/checkpoints/colbert ..\n",
      "#> Joined...\n"
     ]
    }
   ],
   "source": [
    "# training the retrie\n",
    "trainer.train(batch_size=32,\n",
    "              nbits=4, # How many bits will the trained model use when compressing indexes\n",
    "              maxsteps=500000, # Maximum steps hard stop\n",
    "              use_ib_negatives=True, # Use in-batch negative to calculate loss\n",
    "              dim=128, # How many dimensions per embedding. 128 is the default and works well.\n",
    "              learning_rate=5e-6, # Learning rate, small values ([3e-6,3e-5] work best if the base model is BERT-like, 5e-6 is often the sweet spot)\n",
    "              doc_maxlen=128, # Maximum document length. Because of how ColBERT works, smaller chunks (128-256) work very well.\n",
    "              use_relu=False, # Disable ReLU -- doesn't improve performance\n",
    "              warmup_steps=\"auto\", # Defaults to 10%\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
